<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.5">
<title>tensorgrad.functions API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tensorgrad.functions</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tensorgrad.functions.abs"><code class="name flex">
<span>def <span class="ident">abs</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def abs(t: Tensor) -&gt; Tensor:
    return Function(
        _SimpleFunction(&#34;abs&#34;, torch.abs, _SignFunction()),
        (t,),
        {},
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tensorgrad.functions.cross_entropy"><code class="name flex">
<span>def <span class="ident">cross_entropy</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>y: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>dim: str | Iterable[str] | None = None) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cross_entropy(t: Tensor, y: Tensor, dim: DimType = None) -&gt; Tensor:
    # We could make a FunctionSignature for cross entropy, if we want
    # the option of not always expanding it.
    dim = parse_dim(t.edges, dim)
    return -sum(y * log(softmax(t, dim)), dim)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tensorgrad.functions.diag"><code class="name flex">
<span>def <span class="ident">diag</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>new_edges: list[str]) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def diag(t: Tensor, new_edges: list[str]) -&gt; Tensor:
    &#34;&#34;&#34;If `t` is a vector, this creates a diagonal tensor with `t` and creates a diagonal.
    In einsum that means &#34;i-&gt;iii&#34;.
    If `t` is a higher order tensor, with all dims the same size, this extracts the diagonal as a vector.
    In einsum that means &#34;iii-&gt;i&#34;.
    &#34;&#34;&#34;
    # Rename the edges to be distinct from the new edges
    (t,), _renames = _make_distinct(t, used_names=new_edges)

    if not t.shape:
        # We can&#39;t just return t @ Copy(new_edges), since we don&#39;t know the size of the new edges.
        raise ValueError(&#34;Cannot take the diagonal of a scalar.&#34;)

    edges, sizes = zip(*t.shape.items())
    if len(set(sizes)) != 1:
        raise ValueError(&#34;All dimensions must be the same size for the diagonal.&#34;)

    return t @ Copy(sizes[0], *edges, *new_edges)</code></pre>
</details>
<div class="desc"><p>If <code>t</code> is a vector, this creates a diagonal tensor with <code>t</code> and creates a diagonal.
In einsum that means "i-&gt;iii".
If <code>t</code> is a higher order tensor, with all dims the same size, this extracts the diagonal as a vector.
In einsum that means "iii-&gt;i".</p></div>
</dd>
<dt id="tensorgrad.functions.dot"><code class="name flex">
<span>def <span class="ident">dot</span></span>(<span>t1: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>t2: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>dim: str | Iterable[str] | None = None) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dot(t1: Tensor, t2: Tensor, dim: DimType = None) -&gt; Tensor:
    &#34;&#34;&#34;Contract two tensors along the given dimensions, broadcasting over the remaining shared edges.&#34;&#34;&#34;
    dim = parse_dim(t1.edges &amp; t2.edges, dim)
    return sum(t1 * t2, dim)</code></pre>
</details>
<div class="desc"><p>Contract two tensors along the given dimensions, broadcasting over the remaining shared edges.</p></div>
</dd>
<dt id="tensorgrad.functions.exp"><code class="name flex">
<span>def <span class="ident">exp</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exp(t: Tensor) -&gt; Tensor:
    return Function(_ExpFunction(), [t], {})</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tensorgrad.functions.frobenius2"><code class="name flex">
<span>def <span class="ident">frobenius2</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def frobenius2(t: Tensor) -&gt; Tensor:
    return Product([t, t])</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tensorgrad.functions.graph"><code class="name flex">
<span>def <span class="ident">graph</span></span>(<span>dot_graph: str,<br>**vars: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def graph(dot_graph: str, **vars: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    Create a tensor network using a DOT-like graph syntax.

    This function allows you to define tensor networks using a syntax similar to the DOT graph description language.
    It provides a more intuitive way to specify tensor contractions and operations compared to traditional einsum notation.

    Parameters:
    -----------
    dot_graph : str
        A string describing the tensor network using DOT-like syntax. Each line represents an edge or connection
        between tensors. Tensor names are separated by edge specifications, which are hyphen-delimited lists of
        edge names.

    **vars : dict
        Keyword arguments representing the tensors used in the graph. Each key is the tensor name used in the
        dot_graph, and the corresponding value is the actual tensor object.

    Returns:
    --------
    Product
        A Product object representing the tensor network described by the input graph.

    Syntax:
    -------
    - Tensors are represented by their names (e.g., &#39;A&#39;, &#39;B&#39;, &#39;X&#39;).
    - Edge connections are represented by hyphens: &#39;-&#39;. For example, &#39;A -i- B&#39; connects edge &#39;i&#39; of A to edge &#39;i&#39; of B.
    - You can connect edges with different names. For example, &#39;A -i-j- B&#39; connects edge &#39;i&#39; of A to edge &#39;j&#39; of B.
    - Copy tensors are created automatically when a name starting with &#39;*&#39;, like &#39;*3&#39; is used.
    - Lines can be separated by newlines or semicolons.
    - Edges not mentioned in the graph are broadcasted.

    Examples:
    ---------
    1. Matrix multiplication:
    &gt;&gt;&gt; i, j, k = symbols(&#34;i j k&#34;)
    &gt;&gt;&gt; A = Variable(&#34;A&#34;, i, j)
    &gt;&gt;&gt; B = Variable(&#34;B&#34;, j, k)
    &gt;&gt;&gt; result = graph(&#34;A -j- B&#34;, A=A, B=B)

    2. Trace
    &gt;&gt;&gt; i = symbols(&#34;i&#34;)
    &gt;&gt;&gt; X = Variable(&#34;X&#34;, i, j=i)
    &gt;&gt;&gt; result = graph(&#39;&#39;&#39;
    ...     X -i- X
    ...     X -j- X
    ... &#39;&#39;&#39;, X=X)

    3. Element-wise multiplication:
    &gt;&gt;&gt; i, j = symbols(&#34;i j&#34;)
    &gt;&gt;&gt; X = Variable(&#34;X&#34;, i, j)
    &gt;&gt;&gt; Y = Variable(&#34;Y&#34;, i, j)
    &gt;&gt;&gt; result = graph(&#39;&#39;&#39;
    ...     -i- *0 -i- X -j- *1 -j-
    ...         *0 -i- Y -j- *1
    ... &#39;&#39;&#39;, X=X, Y=Y)

    4. Frobenius norm (squared)
    &gt;&gt;&gt; i, j = symbols(&#34;i j&#34;)
    &gt;&gt;&gt; X = Variable(&#34;X&#34;, i, j)
    &gt;&gt;&gt; result = graph(&#39;&#39;&#39;
    ...     X1 -i- X2
    ...     X1 -j- X2
    ... &#39;&#39;&#39;, X1=X, X2=X)

    Raises:
    -------
    ValueError
        If the graph specification is invalid, such as using undefined variables,
        invalid edge names, or inconsistent hyperedge sizes.
    &#34;&#34;&#34;
    vars: dict[str, Tensor] = vars.copy()

    # Parse the graph, converting long lines into a list of edges:
    # st. &#34;A -i-j- B&#34; -&gt; (&#34;A&#34;, &#34;i&#34;, &#34;j&#34;, &#34;B&#34;)
    # and &#34;A -i-&#34; -&gt; (&#34;A&#34;, &#34;i&#34;, &#34;i&#34;, None)
    # and &#34;A -i- B - C&#34; -&gt; [(&#34;A&#34;, &#34;i&#34;, &#34;i&#34;, &#34;B&#34;), (&#34;B&#34;, &#34;i&#34;, &#34;i&#34;, &#34;C&#34;)]
    # and &#34;*1 - *2&#34; -&gt; (&#34;*1&#34;, None, None, &#34;*2&#34;)
    lines = re.split(r&#34;[\n;]&#34;, dot_graph.strip())
    edges = []
    for line in lines:
        parts = line.split()
        last_var = None
        for i in range(len(parts)):
            if parts[i].startswith(&#34;-&#34;):
                _, *edge_names, _ = parts[i].split(&#34;-&#34;)
                if len(edge_names) == 1:
                    edge_names = [edge_names[0]] * 2
                next_var = parts[i + 1] if i + 1 &lt; len(parts) else None
                edges.append((last_var, *edge_names, next_var))
            else:
                last_var = parts[i]

    # Look for edges, e, not mentioned in the graph, and create &#34;X -e- &#34; lines for them
    for v_name, v in vars.items():
        v_unused_edges = set(v.edges)
        for v0, e0, e1, v1 in edges:
            if v0 == v_name:
                v_unused_edges -= {e0}
            if v1 == v_name:
                v_unused_edges -= {e1}
        for e in v_unused_edges:
            edges.append((v_name, e, e, None))

    # Keep track of the hyperedges and their sizes
    # Keys are *i for some i, values are lists of edge names/symbols
    hyperedges: dict[str, list[str]] = defaultdict(list)
    hypersizes: DisjointSets[Any, Symbol] = DisjointSets()

    # Keep track of free edge names we can use
    used_edges: set[str] = {e for v in vars.values() for e in v.edges}
    free_edges: Iterator[str] = (f&#34;e{i}&#34; for i in itertools.count() if f&#34;e{i}&#34; not in used_edges)
    free_hyper_edges: Iterator[str] = (f&#34;h{i}&#34; for i in itertools.count() if f&#34;h{i}&#34; not in used_edges)

    for v0, e0, e1, v1 in edges:
        # Syntax checking
        for v, e in ((v0, e0), (v1, e1)):
            if v and not v.startswith(&#34;*&#34;):
                if v not in vars:
                    raise ValueError(f&#34;Variable {v} not found in vars&#34;)
                if e not in vars[v].edges:
                    raise ValueError(f&#34;Edge {e} not found in variable {v}({vars[v].edges})&#34;)
        if v0 is None and v1 is None:
            raise ValueError(&#34;Cannot have two free edges&#34;)

        # The case &#39;X -i-&#39; or &#39;-i- X&#39;, where an edge is free
        elif v0 is None or v1 is None:
            v, e, eo = (v1, e1, e0) if v0 is None else (v0, e0, e1)
            # In the case &#34;*0 -i-&#34; we add the edge to the hyperedge
            if v.startswith(&#34;*&#34;):
                hyperedges[v].append(e)
            # Otherwise we create a new hyperedge and add the edge to it
            # This allows us to keep track of broadcasting
            else:
                he = (&#34;external&#34;, eo)
                c = next(free_edges)
                if eo not in hyperedges[he]:
                    hyperedges[he].append(eo)
                hyperedges[he].append(c)
                hypersizes[he] = vars[v].shape[e]
                vars[v] = vars[v].rename(**{e: c})

        # The case *0 - *1, that is, two copy edges
        elif v0.startswith(&#34;*&#34;) and v1.startswith(&#34;*&#34;):
            e = next(free_edges)
            hyperedges[v0].append(e)
            hyperedges[v1].append(e)
            hypersizes.union(v0, v1)  # Make sure they have the same size

        # The case *0 -i- X, where we have a single copy edge
        elif v0.startswith(&#34;*&#34;) or v1.startswith(&#34;*&#34;):
            v, e, he = (v0, e0, v1) if v1.startswith(&#34;*&#34;) else (v1, e1, v0)
            c = next(free_edges)
            hyperedges[he].append(c)
            hypersizes[he] = vars[v].shape[e]  # Fails if not compatible
            vars[v] = vars[v].rename(**{e: c})

        # The case where a variable is connected to itself, like &#39;A -i- A&#39;
        elif v0 == v1:
            if e0 == e1:
                raise ValueError(&#34;Cannot have a self loop on a single edge&#34;)
            c0, c1 = next(free_edges), next(free_edges)
            he = next(free_hyper_edges)
            hyperedges[he].extend([c0, c1])
            hypersizes[he] = vars[v0].shape[e0]
            vars[v0] = vars[v0].rename(**{e0: c0, e1: c1})

        # A standard connection between two variables, like &#39;A -i- B&#39;
        else:
            e = next(free_edges)
            vars[v0] = vars[v0].rename(**{e0: e})
            vars[v1] = vars[v1].rename(**{e1: e})

    copies = []
    for he, edges in hyperedges.items():
        size = hypersizes.get(he)
        if size is None:
            raise ValueError(f&#34;Hyperedge {he} has no size&#34;)
        if len(edges) != len(set(edges)):
            raise ValueError(&#34;Hyperedges must be disjoint&#34;)
        copies.append(Copy(size, *edges))

    return Product(copies + list(vars.values()))</code></pre>
</details>
<div class="desc"><p>Create a tensor network using a DOT-like graph syntax.</p>
<p>This function allows you to define tensor networks using a syntax similar to the DOT graph description language.
It provides a more intuitive way to specify tensor contractions and operations compared to traditional einsum notation.</p>
<h2 id="parameters">Parameters:</h2>
<p>dot_graph : str
A string describing the tensor network using DOT-like syntax. Each line represents an edge or connection
between tensors. Tensor names are separated by edge specifications, which are hyphen-delimited lists of
edge names.</p>
<p>**vars : dict
Keyword arguments representing the tensors used in the graph. Each key is the tensor name used in the
dot_graph, and the corresponding value is the actual tensor object.</p>
<h2 id="returns">Returns:</h2>
<p>Product
A Product object representing the tensor network described by the input graph.</p>
<h2 id="syntax">Syntax:</h2>
<ul>
<li>Tensors are represented by their names (e.g., 'A', 'B', 'X').</li>
<li>Edge connections are represented by hyphens: '-'. For example, 'A -i- B' connects edge 'i' of A to edge 'i' of B.</li>
<li>You can connect edges with different names. For example, 'A -i-j- B' connects edge 'i' of A to edge 'j' of B.</li>
<li>Copy tensors are created automatically when a name starting with '<em>', like '</em>3' is used.</li>
<li>Lines can be separated by newlines or semicolons.</li>
<li>Edges not mentioned in the graph are broadcasted.</li>
</ul>
<h2 id="examples">Examples:</h2>
<ol>
<li>Matrix multiplication:</li>
</ol>
<pre><code class="language-python-repl">&gt;&gt;&gt; i, j, k = symbols(&quot;i j k&quot;)
&gt;&gt;&gt; A = Variable(&quot;A&quot;, i, j)
&gt;&gt;&gt; B = Variable(&quot;B&quot;, j, k)
&gt;&gt;&gt; result = graph(&quot;A -j- B&quot;, A=A, B=B)
</code></pre>
<ol>
<li>Trace</li>
</ol>
<pre><code class="language-python-repl">&gt;&gt;&gt; i = symbols(&quot;i&quot;)
&gt;&gt;&gt; X = Variable(&quot;X&quot;, i, j=i)
&gt;&gt;&gt; result = graph('''
...     X -i- X
...     X -j- X
... ''', X=X)
</code></pre>
<ol>
<li>Element-wise multiplication:</li>
</ol>
<pre><code class="language-python-repl">&gt;&gt;&gt; i, j = symbols(&quot;i j&quot;)
&gt;&gt;&gt; X = Variable(&quot;X&quot;, i, j)
&gt;&gt;&gt; Y = Variable(&quot;Y&quot;, i, j)
&gt;&gt;&gt; result = graph('''
...     -i- *0 -i- X -j- *1 -j-
...         *0 -i- Y -j- *1
... ''', X=X, Y=Y)
</code></pre>
<ol>
<li>Frobenius norm (squared)</li>
</ol>
<pre><code class="language-python-repl">&gt;&gt;&gt; i, j = symbols(&quot;i j&quot;)
&gt;&gt;&gt; X = Variable(&quot;X&quot;, i, j)
&gt;&gt;&gt; result = graph('''
...     X1 -i- X2
...     X1 -j- X2
... ''', X1=X, X2=X)
</code></pre>
<h2 id="raises">Raises:</h2>
<p>ValueError
If the graph specification is invalid, such as using undefined variables,
invalid edge names, or inconsistent hyperedge sizes.</p></div>
</dd>
<dt id="tensorgrad.functions.gt"><code class="name flex">
<span>def <span class="ident">gt</span></span>(<span>x: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>y: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gt(x: Tensor, y: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;Returns 1 where x &gt; y, 1/2 where x = y, and 0 where x &lt; y.&#34;&#34;&#34;
    if x.shape != y.shape:
        raise ValueError(f&#34;Inputs must have same shape, got {x.shape=} != {y.shape=}&#34;)
    return (sign(x - y) + 1) / 2</code></pre>
</details>
<div class="desc"><p>Returns 1 where x &gt; y, 1/2 where x = y, and 0 where x &lt; y.</p></div>
</dd>
<dt id="tensorgrad.functions.gt0"><code class="name flex">
<span>def <span class="ident">gt0</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gt0(t: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;Returns a tensor that&#39;s 1 where t is &gt; 0 else 0 elsewhere&#34;&#34;&#34;
    return Function(_Gt0Function(), (t,), {})</code></pre>
</details>
<div class="desc"><p>Returns a tensor that's 1 where t is &gt; 0 else 0 elsewhere</p></div>
</dd>
<dt id="tensorgrad.functions.kronecker"><code class="name flex">
<span>def <span class="ident">kronecker</span></span>(<span>*tensors: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kronecker(*tensors: Tensor) -&gt; Tensor:
    # Basically just rename everything to be distinct, then contraction
    # Note: This function returns the tensor product, which is different from the
    #       Kronecker product as often described in the literature. To get the
    #       Kronecker product you have to flatten the output tensors.
    dis_tensors, _renames = _make_distinct(*tensors)
    return Product(dis_tensors)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tensorgrad.functions.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log(t: Tensor) -&gt; Tensor:
    return Function(_LogFunction(), [t], {})</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tensorgrad.functions.max"><code class="name flex">
<span>def <span class="ident">max</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>dim: str | Iterable[str] | None = None,<br>keepdim: bool = False) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max(t: Tensor, dim: DimType = None, keepdim: bool = False) -&gt; Tensor:
    &#34;&#34;&#34;
    Return the max along &#39;dim&#39;. If dim=(), it&#39;s the global max (0D).
    The derivative is tie-splitting, handled by gt(...).
    &#34;&#34;&#34;

    if isinstance(dim, str):
        dim = (dim,)
    if not dim:
        dim = tuple(t.edges)

    fn = Function(_MaxFunction(dim), [t], {})
    if keepdim:
        fn = fn @ Ones(**{e: t.shape[e] for e in dim})
    return fn</code></pre>
</details>
<div class="desc"><p>Return the max along 'dim'. If dim=(), it's the global max (0D).
The derivative is tie-splitting, handled by gt(&hellip;).</p></div>
</dd>
<dt id="tensorgrad.functions.max_grad"><code class="name flex">
<span>def <span class="ident">max_grad</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>dim: str | Iterable[str] | None = None) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_grad(t: Tensor, dim: DimType = None) -&gt; Tensor:
    &#34;&#34;&#34;
    Tie-splitting subgradient for max. If multiple elements tie for max,
    each gets 1/k of the gradient.
    Maps an order n tensor to an order n tensor.

    Examples:
    (1)   j
        [1 4] -&gt; max({i}) -&gt; [2 4] -&gt; grad({i}) -&gt; [0 1] (broadcasted over j)
      i [2 3]                                      [1 0]

    (2)   j
        [1 4] -&gt; max({i,j}) -&gt; [4] -&gt; grad({i,j}) -&gt; [0 1]
      i [2 3]                                        [0 0]

    (3)   j
        [1 4] -&gt; max({j}) -&gt; [4] -&gt; grad({j}) -&gt; [0   1  ] (broadcasted over i)
      i [3 3]                [3]                 [0.5 0.5]
    &#34;&#34;&#34;
    if isinstance(dim, str):
        dim = {dim}
    # Just as torch.amax, we default to &#34;all dims&#34;.
    if dim is None:
        dim = set(t.edges)

    # All the edges that are consumed by the function are also produced by the function
    func = Function(_MaxGradFunction(dim), [t], {e: t.shape[e] for e in dim})
    assert func.edges == t.edges
    return func</code></pre>
</details>
<div class="desc"><p>Tie-splitting subgradient for max. If multiple elements tie for max,
each gets 1/k of the gradient.
Maps an order n tensor to an order n tensor.</p>
<p>Examples:
(1)
j
[1 4] -&gt; max({i}) -&gt; [2 4] -&gt; grad({i}) -&gt; [0 1] (broadcasted over j)
i [2 3]
[1 0]</p>
<p>(2)
j
[1 4] -&gt; max({i,j}) -&gt; [4] -&gt; grad({i,j}) -&gt; [0 1]
i [2 3]
[0 0]</p>
<p>(3)
j
[1 4] -&gt; max({j}) -&gt; [4] -&gt; grad({j}) -&gt; [0
1
] (broadcasted over i)
i [3 3]
[3]
[0.5 0.5]</p></div>
</dd>
<dt id="tensorgrad.functions.maximum"><code class="name flex">
<span>def <span class="ident">maximum</span></span>(<span>x: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>y: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def maximum(x: Tensor, y: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;Like torch.maximum&#34;&#34;&#34;
    mask = gt(x, y)
    return x * mask + y * (1 - mask)</code></pre>
</details>
<div class="desc"><p>Like torch.maximum</p></div>
</dd>
<dt id="tensorgrad.functions.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>tensor: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>dim: str | Iterable[str] | None = None,<br>keepdims: bool = False) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean(tensor: Tensor, dim: DimType = None, keepdims: bool = False) -&gt; Tensor:
    dim = parse_dim(tensor.edges, dim, none_is=&#34;all&#34;)
    s = sum(tensor, dim, keepdims)
    normalization = 1
    for e in dim:
        normalization @= Copy(tensor.shape[e])
    return s / normalization</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tensorgrad.functions.pairwise_distance"><code class="name flex">
<span>def <span class="ident">pairwise_distance</span></span>(<span>t1: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>t2: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>dim: str | Iterable[str] | None = None) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pairwise_distance(t1: Tensor, t2: Tensor, dim: DimType = None) -&gt; Tensor:
    # If t1 and t2 have shape (x, i), pairwise_distance(t1, t2)_b is
    # ||t1[b] - t2[b]||_2^2
    dim = parse_dim(t1.edges, dim)
    return sum(pow(t1 - t2, 2), dim)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tensorgrad.functions.parse_dim"><code class="name flex">
<span>def <span class="ident">parse_dim</span></span>(<span>tensor_edges: set[str],<br>dim: str | Iterable[str] | None = None,<br>none_is: str = 'error') ‑> set[str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_dim(tensor_edges: set[str], dim: DimType = None, none_is: str = &#34;error&#34;) -&gt; set[str]:
    if dim is None:
        if none_is == &#34;all&#34;:
            dim = frozenset(tensor_edges)
        else:
            raise ValueError(&#34;Dimension(s) must be set&#34;)
    if isinstance(dim, str):
        dim = {dim}
    if not isinstance(dim, set):
        dim = set(dim)
    if not dim.issubset(tensor_edges):
        raise ValueError(f&#34;{dim=} must be a subset of {tensor_edges}&#34;)
    return dim</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tensorgrad.functions.pow"><code class="name flex">
<span>def <span class="ident">pow</span></span>(<span>tensor: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>k: int | fractions.Fraction) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pow(tensor: Tensor, k: int | Fraction) -&gt; Tensor:
    &#34;&#34;&#34;Elementwise t^k&#34;&#34;&#34;
    if k == 0:
        return Ones(**tensor.shape)
    if k == 1:
        return tensor
    return Function(_PowerFunction(k), [tensor], {})</code></pre>
</details>
<div class="desc"><p>Elementwise t^k</p></div>
</dd>
<dt id="tensorgrad.functions.relu"><code class="name flex">
<span>def <span class="ident">relu</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def relu(t: Tensor) -&gt; Tensor:
    return Function(
        _SimpleFunction(&#34;relu&#34;, torch.relu, _Gt0Function()),
        (t,),
        {},
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tensorgrad.functions.sigmoid"><code class="name flex">
<span>def <span class="ident">sigmoid</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sigmoid(t: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;Implements the sigmoid function, 1/(1 + e^-t)&#34;&#34;&#34;
    return 1 / (1 + exp(-t))</code></pre>
</details>
<div class="desc"><p>Implements the sigmoid function, 1/(1 + e^-t)</p></div>
</dd>
<dt id="tensorgrad.functions.sign"><code class="name flex">
<span>def <span class="ident">sign</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sign(t: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;Returns a tensor that&#39;s
    a)  1 where t &gt; 0
    a)  0 where t = 0
    a) -1 where t &lt; 0
    like torch.sign&#34;&#34;&#34;
    return Function(_SignFunction(), (t,), {})</code></pre>
</details>
<div class="desc"><p>Returns a tensor that's
a)
1 where t &gt; 0
a)
0 where t = 0
a) -1 where t &lt; 0
like torch.sign</p></div>
</dd>
<dt id="tensorgrad.functions.softmax"><code class="name flex">
<span>def <span class="ident">softmax</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>dim: str | Iterable[str] | None = None) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def softmax(t: Tensor, dim: DimType = None) -&gt; Tensor:
    dim = parse_dim(t.edges, dim)
    return Function(_SoftmaxFunction(dim), [t], {e: t.shape[e] for e in dim})</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tensorgrad.functions.sqrt"><code class="name flex">
<span>def <span class="ident">sqrt</span></span>(<span>tensor: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sqrt(tensor: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;Elementwise sqrt&#34;&#34;&#34;
    return pow(tensor, Fraction(1, 2))</code></pre>
</details>
<div class="desc"><p>Elementwise sqrt</p></div>
</dd>
<dt id="tensorgrad.functions.sum"><code class="name flex">
<span>def <span class="ident">sum</span></span>(<span>tensor: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>dim: str | Iterable[str] | None = None,<br>keepdims: bool = False) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sum(tensor: Tensor, dim: DimType = None, keepdims: bool = False) -&gt; Tensor:
    &#34;&#34;&#34;Sum the tensor over the given dimensions.&#34;&#34;&#34;
    dim = parse_dim(tensor.edges, dim, none_is=&#34;all&#34;)
    out = Product([tensor] + [Copy(tensor.shape[e], e) for e in dim])
    # Optionally broadcast back to orignal shape
    if keepdims:
        return out @ Ones(**{e: tensor.shape[e] for e in dim})
    return out</code></pre>
</details>
<div class="desc"><p>Sum the tensor over the given dimensions.</p></div>
</dd>
<dt id="tensorgrad.functions.symmetrize"><code class="name flex">
<span>def <span class="ident">symmetrize</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>signed: bool = False) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def symmetrize(t: Tensor, signed: bool = False) -&gt; Tensor:
    &#34;&#34;&#34;Sum over all permutations of the edges.&#34;&#34;&#34;
    edges = list(t.edges)
    res = []
    weights = []
    for perm in itertools.permutations(edges):
        res.append(t.rename(**dict(zip(edges, perm))))
        if signed:
            weights.append(1 if _is_even_permutation(perm) else -1)
        else:
            weights.append(1)
    return Sum(res, weights)</code></pre>
</details>
<div class="desc"><p>Sum over all permutations of the edges.</p></div>
</dd>
<dt id="tensorgrad.functions.tanh"><code class="name flex">
<span>def <span class="ident">tanh</span></span>(<span>t: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tanh(t: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;Implements the tanh function, (e^t - e^(-t))/(e^t + e^(-t))&#34;&#34;&#34;
    e = exp(t)
    em = exp(-t)
    return (e - em) / (e + em)</code></pre>
</details>
<div class="desc"><p>Implements the tanh function, (e^t - e^(-t))/(e^t + e^(-t))</p></div>
</dd>
<dt id="tensorgrad.functions.taylor"><code class="name flex">
<span>def <span class="ident">taylor</span></span>(<span>f: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>wrt: <a title="tensorgrad.tensor.Variable" href="tensor.html#tensorgrad.tensor.Variable">Variable</a>,<br>eps: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>,<br>n: int) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def taylor(f: Tensor, wrt: Variable, eps: Tensor, n: int) -&gt; Tensor:
    &#34;&#34;&#34;Return the nth order Taylor approximation of f at x+eps.&#34;&#34;&#34;
    if eps.edges != wrt.edges:
        raise ValueError(&#34;eps must have the same edges as wrt.&#34;)
    total = f
    for i in range(1, n + 1):
        connection_names = _unused_edge_names(wrt.edges, f.edges)
        fg = f.grad(wrt, new_names=connection_names)
        scaled_eps = eps.rename(**connection_names)
        total = total + (fg @ scaled_eps) * (1.0 / math.factorial(i))
    return total</code></pre>
</details>
<div class="desc"><p>Return the nth order Taylor approximation of f at x+eps.</p></div>
</dd>
<dt id="tensorgrad.functions.trace"><code class="name flex">
<span>def <span class="ident">trace</span></span>(<span>tensor: <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a>) ‑> <a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def trace(tensor: Tensor) -&gt; Tensor:
    if not tensor.edges:
        return tensor
    return diag(tensor, [])</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tensorgrad.functions.Convolution"><code class="flex name class">
<span>class <span class="ident">Convolution</span></span>
<span>(</span><span>*shape0: sympy.core.symbol.Symbol, **shape1: sympy.core.symbol.Symbol)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Convolution(Constant):
    def __init__(self, *shape0: Symbol, _symmetries: set[frozenset[str]] = None, **shape1: Symbol):
        &#34;&#34;&#34;
        A Convolution is a 3-tensor such that C[i,j,k] = 1 if i=j+k and 0 otherwise.
        Typically the first argument (i) is the input dim, and two others are the kernel and output dims.

        Use Convolution(win, kw, wout) to represent a 1-Dconvolution with
        input size win, kernel size kw, and output size wout.
        wout will be win - kw + 1.
        For 2D convolution, use Convolution(win, kw, wout) @ Convolution(hin, kh, hout).

        Output shape (patches, dim) where dim = channels * kernel_width * kernel_height
        But that&#39;s where I&#39;m arguing that we don&#39;t need to flatten the channels unto the output
        we can just keep it broadcasted and people can flatten it if they want.
        I don&#39;t know why torch.Unfold doesn&#39;t do it this way, but presumably there&#39;s some performance hit?

        width_in = 6
        [x x x x x x]
        [1 0 0 - - -] [0 1 0 - - -] [0 0 1 - - -]
        [- 1 0 0 - -] [- 0 1 0 - -] [- 0 0 1 - -]
        [- - 1 0 0 -] [- - 0 1 0 -] [- - 0 0 1 -]
        [- - - 1 0 0] [- - - 0 1 0] [- - - 0 0 1]
        width_out = 4
        kw = 3

        (width_out, kw, width_in)
        [1 0 0 - - -]
        [0 1 0 - - -]
        [0 0 1 - - -]

        [- 1 0 0 - -]
        [- 0 1 0 - -]
        [- 0 0 1 - -]

        [- - 1 0 0 -]
        [- - 0 1 0 -]
        [- - 0 0 1 -]

        [- - - 1 0 0]
        [- - - 0 1 0]
        [- - - 0 0 1]

        width_in = 5
        height_in = 3
        [x x x x x]
        [x x x x x]
        [x x x x x]
        [1 0 - - -]
        [0 1 - - -]

        [1 0 0 - - -] [0 1 0 - - -] [0 0 1 - - -]
        [- 1 0 0 - -] [- 0 1 0 - -] [- 0 0 1 - -]
        [- - 1 0 0 -] [- - 0 1 0 -] [- - 0 0 1 -]
        [- - - 1 0 0] [- - - 0 1 0] [- - - 0 0 1]
        width_out = 4
        kw = 3
        &#34;&#34;&#34;
        shape = self._check_shape(shape0, shape1)
        assert len(shape) == 3, &#34;Convolution must have exactly 3 edges: input, kernel, output&#34;
        (e0, _s0), (e1, s1), (e2, s2) = shape.items()
        if s1 == s2:
            assert len({s1, s2}) == 1, &#34;Kernel and output size must be the same&#34;
        symmetries = {frozenset([e1, e2]), frozenset([e0])} if s1 == s2 else None
        super().__init__(_symmetries=symmetries, **shape)
        self.input_name, self.kernel_name, self.output_name = e0, e1, e2

    def __repr__(self) -&gt; str:
        return f&#34;Convolution({self.input_name}, {self.kernel_name}, {self.output_name})&#34;

    def rename(self, **kwargs: str) -&gt; &#34;Tensor&#34;:
        kwargs = self._check_rename(kwargs)
        return Convolution(**{kwargs.get(k, k): v for k, v in self.shape.items()})

    def evaluate(
        self,
        values: dict[&#34;Variable&#34;, torch.Tensor],
        dims: dict[Symbol, int] | None = None,
    ) -&gt; torch.Tensor:
        if not self.edges:
            return torch.tensor(1.0)
        w_in = dims[self.shape[self.input_name]]
        k_size = dims[self.shape[self.kernel_name]]

        # TODO: Right now we assume w_in and k_size are given, and compute the output size
        # manually. However, we could use any of the two to compute the third.
        w_out = w_in - k_size + 1

        # Check consistency
        if w_out_given := dims.get(self.shape[self.output_name]) is not None:
            assert w_out == w_out_given, (
                f&#34;Convolution expects its output dim to be win - kw + 1, &#34;
                f&#34;but got {w_in} - {k_size} + 1 = {w_out} vs. {w_out_given}.&#34;
            )

        # Make a tensor T, such that T[i,j,k] = 1 iff i=j+k
        res = torch.zeros(w_in, k_size, w_out)
        for k in range(w_out):
            for j in range(k_size):
                res[k + j, j, k] = 1
        return res.rename(self.input_name, self.kernel_name, self.output_name)</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>A Convolution is a 3-tensor such that C[i,j,k] = 1 if i=j+k and 0 otherwise.
Typically the first argument (i) is the input dim, and two others are the kernel and output dims.</p>
<p>Use Convolution(win, kw, wout) to represent a 1-Dconvolution with
input size win, kernel size kw, and output size wout.
wout will be win - kw + 1.
For 2D convolution, use Convolution(win, kw, wout) @ Convolution(hin, kh, hout).</p>
<p>Output shape (patches, dim) where dim = channels * kernel_width * kernel_height
But that's where I'm arguing that we don't need to flatten the channels unto the output
we can just keep it broadcasted and people can flatten it if they want.
I don't know why torch.Unfold doesn't do it this way, but presumably there's some performance hit?</p>
<p>width_in = 6
[x x x x x x]
[1 0 0 - - -] [0 1 0 - - -] [0 0 1 - - -]
[- 1 0 0 - -] [- 0 1 0 - -] [- 0 0 1 - -]
[- - 1 0 0 -] [- - 0 1 0 -] [- - 0 0 1 -]
[- - - 1 0 0] [- - - 0 1 0] [- - - 0 0 1]
width_out = 4
kw = 3</p>
<p>(width_out, kw, width_in)
[1 0 0 - - -]
[0 1 0 - - -]
[0 0 1 - - -]</p>
<p>[- 1 0 0 - -]
[- 0 1 0 - -]
[- 0 0 1 - -]</p>
<p>[- - 1 0 0 -]
[- - 0 1 0 -]
[- - 0 0 1 -]</p>
<p>[- - - 1 0 0]
[- - - 0 1 0]
[- - - 0 0 1]</p>
<p>width_in = 5
height_in = 3
[x x x x x]
[x x x x x]
[x x x x x]
[1 0 - - -]
[0 1 - - -]</p>
<p>[1 0 0 - - -] [0 1 0 - - -] [0 0 1 - - -]
[- 1 0 0 - -] [- 0 1 0 - -] [- 0 0 1 - -]
[- - 1 0 0 -] [- - 0 1 0 -] [- - 0 0 1 -]
[- - - 1 0 0] [- - - 0 1 0] [- - - 0 0 1]
width_out = 4
kw = 3</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="tensorgrad.tensor.Constant" href="tensor.html#tensorgrad.tensor.Constant">Constant</a></li>
<li><a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="tensorgrad.tensor.Constant" href="tensor.html#tensorgrad.tensor.Constant">Constant</a></b></code>:
<ul class="hlist">
<li><code><a title="tensorgrad.tensor.Constant.depends_on" href="tensor.html#tensorgrad.tensor.Tensor.depends_on">depends_on</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.edge_structural_graph" href="tensor.html#tensorgrad.tensor.Tensor.edge_structural_graph">edge_structural_graph</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.edges" href="tensor.html#tensorgrad.tensor.Tensor.edges">edges</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.evaluate" href="tensor.html#tensorgrad.tensor.Tensor.evaluate">evaluate</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.full_simplify" href="tensor.html#tensorgrad.tensor.Tensor.full_simplify">full_simplify</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.grad" href="tensor.html#tensorgrad.tensor.Tensor.grad">grad</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.graph_to_string" href="tensor.html#tensorgrad.tensor.Tensor.graph_to_string">graph_to_string</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.isomorphisms" href="tensor.html#tensorgrad.tensor.Tensor.isomorphisms">isomorphisms</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.order" href="tensor.html#tensorgrad.tensor.Tensor.order">order</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.rename" href="tensor.html#tensorgrad.tensor.Tensor.rename">rename</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.simplify" href="tensor.html#tensorgrad.tensor.Tensor.simplify">simplify</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.structural_graph" href="tensor.html#tensorgrad.tensor.Tensor.structural_graph">structural_graph</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.symmetries" href="tensor.html#tensorgrad.tensor.Tensor.symmetries">symmetries</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.weisfeiler_lehman" href="tensor.html#tensorgrad.tensor.Tensor.weisfeiler_lehman">weisfeiler_lehman</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="tensorgrad.functions.Flatten"><code class="flex name class">
<span>class <span class="ident">Flatten</span></span>
<span>(</span><span>*shape0: sympy.core.symbol.Symbol, **shape1: sympy.core.symbol.Symbol)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Flatten(Constant):
    def __init__(self, *shape0: Symbol, _symmetries: set[frozenset[str]] = None, **shape1: Symbol):
        shape = self._check_shape(shape0, shape1)
        *self.input_edges, self.output_edge = shape.keys()
        assert len(shape) &gt;= 2, &#34;Flatten must have at least 2 edges&#34;
        # We have symmetry between all input edges, as long as they are all the same size
        groups = defaultdict(list)
        for e in self.input_edges:
            groups[shape[e]].append(e)
        groups[self.output_edge].append(self.output_edge)
        super().__init__(_symmetries=set(map(frozenset, groups.values())), **shape)

    def __repr__(self) -&gt; str:
        return f&#34;Flatten({self.input_edges}, {self.output_edge})&#34;

    def __hash__(self) -&gt; int:
        return hash((type(self).__name__, len(self.edges)))

    def rename(self, **kwargs: str) -&gt; &#34;Tensor&#34;:
        kwargs = self._check_rename(kwargs)
        return Flatten(**{kwargs.get(k, k): v for k, v in self.shape.items()})

    def evaluate(
        self,
        values: dict[&#34;Variable&#34;, torch.Tensor],
        dims: dict[Symbol, int] | None = None,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Produces a tensor of shape [*input_dims, output_dim] where
        output_dim = product of all input_dims.

        For each multi-index (i1, i2, ..., iN),
        we set res[i1, i2, ..., iN, flatten(i1, i2, ..., iN)] = 1
        and zero otherwise.
        &#34;&#34;&#34;
        # Collect the sizes of the edges
        input_dims = [dims[self.shape[e]] for e in self.input_edges]
        output_dim = dims.get(self.shape[self.output_edge])

        # Check consistency: output_dim should be the product of input_dims
        prod_in = 1
        for d in input_dims:
            prod_in *= d

        if output_dim is not None:
            assert prod_in == output_dim, (
                f&#34;Flatten expects its output dim == product of input dims, &#34;
                f&#34;but got product({input_dims}) = {prod_in} vs. {output_dim}.&#34;
            )
        else:
            output_dim = prod_in

        # Build an identity matrix and reshape it so that row indices
        # become the multi-index (i1, i2, ..., iN),
        # and the column index becomes the flattened coordinate.
        eye = torch.eye(output_dim)  # shape = [output_dim, output_dim]
        res = eye.reshape(*input_dims, output_dim)  # shape = [*input_dims, output_dim]
        return res.rename(*self.input_edges, self.output_edge)</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p>
<p>A constant tensor with the given edges.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>edges</code></strong></dt>
<dd>The names of the edges of this constant tensor.</dd>
<dt><strong><code>link</code></strong></dt>
<dd>An optional variable that this tensor is associated with, used to compute edge dimensions.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="tensorgrad.tensor.Constant" href="tensor.html#tensorgrad.tensor.Constant">Constant</a></li>
<li><a title="tensorgrad.tensor.Tensor" href="tensor.html#tensorgrad.tensor.Tensor">Tensor</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="tensorgrad.functions.Flatten.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self,<br>values: dict['Variable', torch.Tensor],<br>dims: dict[sympy.core.symbol.Symbol, int] | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(
    self,
    values: dict[&#34;Variable&#34;, torch.Tensor],
    dims: dict[Symbol, int] | None = None,
) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    Produces a tensor of shape [*input_dims, output_dim] where
    output_dim = product of all input_dims.

    For each multi-index (i1, i2, ..., iN),
    we set res[i1, i2, ..., iN, flatten(i1, i2, ..., iN)] = 1
    and zero otherwise.
    &#34;&#34;&#34;
    # Collect the sizes of the edges
    input_dims = [dims[self.shape[e]] for e in self.input_edges]
    output_dim = dims.get(self.shape[self.output_edge])

    # Check consistency: output_dim should be the product of input_dims
    prod_in = 1
    for d in input_dims:
        prod_in *= d

    if output_dim is not None:
        assert prod_in == output_dim, (
            f&#34;Flatten expects its output dim == product of input dims, &#34;
            f&#34;but got product({input_dims}) = {prod_in} vs. {output_dim}.&#34;
        )
    else:
        output_dim = prod_in

    # Build an identity matrix and reshape it so that row indices
    # become the multi-index (i1, i2, ..., iN),
    # and the column index becomes the flattened coordinate.
    eye = torch.eye(output_dim)  # shape = [output_dim, output_dim]
    res = eye.reshape(*input_dims, output_dim)  # shape = [*input_dims, output_dim]
    return res.rename(*self.input_edges, self.output_edge)</code></pre>
</details>
<div class="desc"><p>Produces a tensor of shape [*input_dims, output_dim] where
output_dim = product of all input_dims.</p>
<p>For each multi-index (i1, i2, &hellip;, iN),
we set res[i1, i2, &hellip;, iN, flatten(i1, i2, &hellip;, iN)] = 1
and zero otherwise.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="tensorgrad.tensor.Constant" href="tensor.html#tensorgrad.tensor.Constant">Constant</a></b></code>:
<ul class="hlist">
<li><code><a title="tensorgrad.tensor.Constant.depends_on" href="tensor.html#tensorgrad.tensor.Tensor.depends_on">depends_on</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.edge_structural_graph" href="tensor.html#tensorgrad.tensor.Tensor.edge_structural_graph">edge_structural_graph</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.edges" href="tensor.html#tensorgrad.tensor.Tensor.edges">edges</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.full_simplify" href="tensor.html#tensorgrad.tensor.Tensor.full_simplify">full_simplify</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.grad" href="tensor.html#tensorgrad.tensor.Tensor.grad">grad</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.graph_to_string" href="tensor.html#tensorgrad.tensor.Tensor.graph_to_string">graph_to_string</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.isomorphisms" href="tensor.html#tensorgrad.tensor.Tensor.isomorphisms">isomorphisms</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.order" href="tensor.html#tensorgrad.tensor.Tensor.order">order</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.rename" href="tensor.html#tensorgrad.tensor.Tensor.rename">rename</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.simplify" href="tensor.html#tensorgrad.tensor.Tensor.simplify">simplify</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.structural_graph" href="tensor.html#tensorgrad.tensor.Tensor.structural_graph">structural_graph</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.symmetries" href="tensor.html#tensorgrad.tensor.Tensor.symmetries">symmetries</a></code></li>
<li><code><a title="tensorgrad.tensor.Constant.weisfeiler_lehman" href="tensor.html#tensorgrad.tensor.Tensor.weisfeiler_lehman">weisfeiler_lehman</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tensorgrad" href="index.html">tensorgrad</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="tensorgrad.functions.abs" href="#tensorgrad.functions.abs">abs</a></code></li>
<li><code><a title="tensorgrad.functions.cross_entropy" href="#tensorgrad.functions.cross_entropy">cross_entropy</a></code></li>
<li><code><a title="tensorgrad.functions.diag" href="#tensorgrad.functions.diag">diag</a></code></li>
<li><code><a title="tensorgrad.functions.dot" href="#tensorgrad.functions.dot">dot</a></code></li>
<li><code><a title="tensorgrad.functions.exp" href="#tensorgrad.functions.exp">exp</a></code></li>
<li><code><a title="tensorgrad.functions.frobenius2" href="#tensorgrad.functions.frobenius2">frobenius2</a></code></li>
<li><code><a title="tensorgrad.functions.graph" href="#tensorgrad.functions.graph">graph</a></code></li>
<li><code><a title="tensorgrad.functions.gt" href="#tensorgrad.functions.gt">gt</a></code></li>
<li><code><a title="tensorgrad.functions.gt0" href="#tensorgrad.functions.gt0">gt0</a></code></li>
<li><code><a title="tensorgrad.functions.kronecker" href="#tensorgrad.functions.kronecker">kronecker</a></code></li>
<li><code><a title="tensorgrad.functions.log" href="#tensorgrad.functions.log">log</a></code></li>
<li><code><a title="tensorgrad.functions.max" href="#tensorgrad.functions.max">max</a></code></li>
<li><code><a title="tensorgrad.functions.max_grad" href="#tensorgrad.functions.max_grad">max_grad</a></code></li>
<li><code><a title="tensorgrad.functions.maximum" href="#tensorgrad.functions.maximum">maximum</a></code></li>
<li><code><a title="tensorgrad.functions.mean" href="#tensorgrad.functions.mean">mean</a></code></li>
<li><code><a title="tensorgrad.functions.pairwise_distance" href="#tensorgrad.functions.pairwise_distance">pairwise_distance</a></code></li>
<li><code><a title="tensorgrad.functions.parse_dim" href="#tensorgrad.functions.parse_dim">parse_dim</a></code></li>
<li><code><a title="tensorgrad.functions.pow" href="#tensorgrad.functions.pow">pow</a></code></li>
<li><code><a title="tensorgrad.functions.relu" href="#tensorgrad.functions.relu">relu</a></code></li>
<li><code><a title="tensorgrad.functions.sigmoid" href="#tensorgrad.functions.sigmoid">sigmoid</a></code></li>
<li><code><a title="tensorgrad.functions.sign" href="#tensorgrad.functions.sign">sign</a></code></li>
<li><code><a title="tensorgrad.functions.softmax" href="#tensorgrad.functions.softmax">softmax</a></code></li>
<li><code><a title="tensorgrad.functions.sqrt" href="#tensorgrad.functions.sqrt">sqrt</a></code></li>
<li><code><a title="tensorgrad.functions.sum" href="#tensorgrad.functions.sum">sum</a></code></li>
<li><code><a title="tensorgrad.functions.symmetrize" href="#tensorgrad.functions.symmetrize">symmetrize</a></code></li>
<li><code><a title="tensorgrad.functions.tanh" href="#tensorgrad.functions.tanh">tanh</a></code></li>
<li><code><a title="tensorgrad.functions.taylor" href="#tensorgrad.functions.taylor">taylor</a></code></li>
<li><code><a title="tensorgrad.functions.trace" href="#tensorgrad.functions.trace">trace</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tensorgrad.functions.Convolution" href="#tensorgrad.functions.Convolution">Convolution</a></code></h4>
</li>
<li>
<h4><code><a title="tensorgrad.functions.Flatten" href="#tensorgrad.functions.Flatten">Flatten</a></code></h4>
<ul class="">
<li><code><a title="tensorgrad.functions.Flatten.evaluate" href="#tensorgrad.functions.Flatten.evaluate">evaluate</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.5</a>.</p>
</footer>
</body>
</html>
