<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TensorGrad Documentation</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css">
    <style>
        body { max-width: 800px; margin: 0 auto; padding: 20px; }
        pre { background: #f5f5f5; padding: 15px; border-radius: 5px; }
        code { background: #f5f5f5; padding: 2px 5px; border-radius: 3px; }
        .warning { background: #fff3cd; padding: 15px; border-radius: 5px; }
        nav { margin: 20px 0; }
        nav a { margin-right: 15px; }
        img { max-width: 100%; height: auto; }
    </style>
</head>
<body>
    <h1>TensorGrad Documentation</h1>
    
    <nav>
        <a href="index.html">Home</a>
        <a href="guide.html">User Guide</a>
        <a href="api.html">API Reference</a>
    </nav>

    <p>TensorGrad is a tensor & deep learning framework that combines PyTorch's computational power with symbolic manipulation capabilities. It provides powerful visualization tools for tensor networks and automatic differentiation.</p>

    <h2>Key Features</h2>
    <ul>
        <li>Symbolic tensor manipulation with Penrose graphical notation</li>
        <li>Automatic differentiation with visual step-by-step derivations</li>
        <li>Integration with PyTorch Named Tensors</li>
        <li>Visualization tools for tensor networks</li>
        <li>Support for expectations of Gaussian tensors</li>
    </ul>

    <h2>Installation</h2>
    <pre><code>pip install tensorgrad</code></pre>

    <div class="warning">
        <p><strong>Note:</strong> For visualizations, additional LaTeX packages are required:</p>
        <pre><code>apt-get install texlive-luatex
apt-get install texlive-latex-extra
apt-get install texlive-fonts-extra
apt-get install poppler-utils</code></pre>
    </div>

    <h2>Quick Example</h2>
    <pre><code>from tensorgrad import Variable
import tensorgrad.functions as F
from sympy import symbols

# Create symbolic dimensions
b, x, y = symbols("b x y")

# Define variables
X = Variable("X", b, x)
Y = Variable("Y", b, y)
W = Variable("W", x, y)

# Create computation
XWmY = X @ W - Y
l2 = XWmY @ XWmY

# Compute gradient
grad = l2.grad(W)

# The result can be:
# - Evaluated with actual tensors
# - Visualized as a tensor diagram
# - Converted to PyTorch code</code></pre>

    <h2>Learn More</h2>
    <ul>
        <li><a href="guide.html">User Guide</a> - Detailed explanations and examples</li>
        <li><a href="api.html">API Reference</a> - Complete API documentation</li>
        <li><a href="https://github.com/thomasahle/tensorgrad/blob/main/paper/cookbook.pdf">Tensor Cookbook (PDF)</a> - In-depth theory and applications</li>
    </ul>

</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TensorGrad User Guide</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css">
    <style>
        body { max-width: 800px; margin: 0 auto; padding: 20px; }
        pre { background: #f5f5f5; padding: 15px; border-radius: 5px; }
        code { background: #f5f5f5; padding: 2px 5px; border-radius: 3px; }
        .note { background: #e7f5ff; padding: 15px; border-radius: 5px; }
        nav { margin: 20px 0; }
        nav a { margin-right: 15px; }
        img { max-width: 100%; height: auto; }
    </style>
</head>
<body>
    <h1>TensorGrad User Guide</h1>

    <nav>
        <a href="index.html">Home</a>
        <a href="guide.html">User Guide</a>
        <a href="api.html">API Reference</a>
    </nav>

    <h2>Core Concepts</h2>

    <h3>Variables and Dimensions</h3>
    <p>In TensorGrad, tensors are created with symbolic dimensions using SymPy symbols:</p>
    <pre><code>from sympy import symbols
from tensorgrad import Variable

# Create dimension symbols
i, j = symbols("i j")

# Create a matrix variable
X = Variable("X", i, j)

# Create a symmetric matrix
S = Variable("S", i=i, j=i).with_symmetries("i j")</code></pre>

    <h3>Basic Operations</h3>
    <p>TensorGrad supports standard tensor operations:</p>
    <pre><code>import tensorgrad.functions as F

# Matrix multiplication (using @)
C = A @ B

# Element-wise operations
y = F.exp(x)
z = F.relu(x)

# Reductions
s = F.sum(x, ["i"])
m = F.max(x, ["i"])

# Softmax
p = F.softmax(x, dim="i")</code></pre>

    <h3>Gradients</h3>
    <p>Computing gradients is a core feature:</p>
    <pre><code># L2 loss example
XWmY = X @ W - Y
loss = F.frobenius2(XWmY)  # ||XW - Y||²
grad = loss.grad(W)

# Cross entropy example
logits = Variable("logits", ["C"])
target = Variable("target", ["C"])
ce = F.cross_entropy(logits, target, dim="C")
grad = ce.grad(logits)</code></pre>

    <h3>Expectations</h3>
    <p>TensorGrad can compute expectations of arbitrary functions with respect to Gaussian tensors:</p>
    <pre><code>from tensorgrad.extras.expectation import Expectation

# Define mean and covariance
mu = Variable("mu", i, j)
covar = Variable("covar", i, j, i2=i, j2=j)

# Compute expectation
E = Expectation(expr, wrt=X, mu=mu, covar=covar)</code></pre>

    <h2>Advanced Features</h2>

    <h3>Tensor Networks</h3>
    <p>TensorGrad provides a graph-based syntax for complex tensor operations:</p>
    <pre><code># Using graph syntax for tensor contractions
expr = F.graph("""
    A -i- X0 -j- B
    X0 -k- C
    B -l- D
""", A=A, X0=X, B=B, C=C, D=D)</code></pre>

    <h3>Neural Network Components</h3>
    <p>Common neural network operations are supported:</p>
    <pre><code># Convolution
data = Variable("data", ["b", "c", "w", "h"])
unfold = Convolution("w", "j", "w2") @ Convolution("h", "i", "h2")
kernel = Variable("kernel", ["c", "i", "j", "c2"])
conv = data @ unfold @ kernel

# Attention mechanism
query = W_q @ X
key = W_k @ X
value = W_v @ X
logits = F.dot(query, key, ["inner"])
attention = F.softmax(logits, dim="seq_k")</code></pre>

    <div class="note">
        <h3>Integration with PyTorch</h3>
        <p>TensorGrad expressions can be evaluated using PyTorch tensors:</p>
        <pre><code>import torch

# Create actual tensors
values = {
    X: torch.randn(3, 4, names=('i', 'j')),
    W: torch.randn(4, 2, names=('j', 'k'))
}

# Evaluate the expression
result = expr.evaluate(values)</code></pre>
    </div>

    <h2>Visualization</h2>
    <p>TensorGrad can visualize tensor networks in multiple formats:</p>
    <pre><code># Generate TikZ diagram
from tensorgrad.serializers.to_tikz import to_tikz
tikz_code = to_tikz(expr)

# Generate Graphviz diagram
from tensorgrad.serializers.to_graphviz import to_graphviz
dot_code = to_graphviz(expr)</code></pre>

    <p>For complete API documentation, see the <a href="api.html">API Reference</a>.</p>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TensorGrad API Reference</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css">
    <style>
        body { max-width: 800px; margin: 0 auto; padding: 20px; }
        pre { background: #f5f5f5; padding: 15px; border-radius: 5px; }
        code { background: #f5f5f5; padding: 2px 5px; border-radius: 3px; }
        .method { margin: 20px 0; padding: 15px; border-left: 3px solid #0366d6; }
        nav { margin: 20px 0; }
        nav a { margin-right: 15px; }
    </style>
</head>
<body>
    <h1>TensorGrad API Reference</h1>

    <nav>
        <a href="index.html">Home</a>
        <a href="guide.html">User Guide</a>
        <a href="api.html">API Reference</a>
    </nav>

    <h2>Core Classes</h2>

    <h3>Variable</h3>
    <div class="method">
        <h4>Variable(name, *shape0, _symmetries=None, _orig=None, **shape1)</h4>
        <p>Creates a variable tensor with the given edges and optional symmetries.</p>
        <pre><code># Example:
i, j = symbols("i j")
x = Variable("x", i, j)  # Matrix
y = Variable("y", i=i, j=i).with_symmetries("i j")  # Symmetric matrix</code></pre>
    </div>

    <h3>Functions</h3>
    <div class="method">
        <h4>frobenius2(t: Tensor) -> Tensor</h4>
        <p>Computes the squared Frobenius norm ||t||².</p>
    </div>

    <div class="method">
        <h4>symmetrize(t: Tensor) -> Tensor</h4>
        <p>Returns (t + t.T)/2.</p>
    </div>

    <div class="method">
        <h4>einsum(tensors, output_edges)</h4>
        <p>Einstein summation over tensors.</p>
    </div>

    <div class="method">
        <h4>dot(t1: Tensor, t2: Tensor, dims: list[str]) -> Tensor</h4>
        <p>Computes dot product along specified dimensions.</p>
    </div>

    <div class="method">
        <h4>trace(tensor: Tensor) -> Tensor</h4>
        <p>Computes the trace of a square tensor.</p>
    </div>

    <div class="method">
        <h4>sum(tensor: Tensor, edges: list[str] = None, keepdims=False) -> Tensor</h4>
        <p>Sums over specified edges.</p>
    </div>

    <div class="method">
        <h4>mean(tensor: Tensor, edges: list[str] = None, keepdims=False) -> Tensor</h4>
        <p>Takes mean over specified edges.</p>
    </div>

    <h3>Neural Network Functions</h3>
    <div class="method">
        <h4>relu(t: Tensor) -> Tensor</h4>
        <p>Applies ReLU activation function.</p>
    </div>

    <div class="method">
        <h4>softmax(t: Tensor, dims: list[str]) -> Tensor</h4>
        <p>Applies softmax along specified dimensions.</p>
    </div>

    <div class="method">
        <h4>cross_entropy(logits: Tensor, target: Tensor, dim: str) -> Tensor</h4>
        <p>Computes cross entropy loss.</p>
    </div>

    <h3>Expectation</h3>
    <div class="method">
        <h4>Expectation(tensor: Tensor, wrt: Variable, mu: Tensor = None, covar: Tensor = None)</h4>
        <p>Computes expectation of tensor with respect to Gaussian variable.</p>
        <pre><code># Example:
mu = Variable("mu", i, j)
covar = Variable("covar", i, j, i2=i, j2=j)
E = Expectation(expr, wrt=X, mu=mu, covar=covar)</code></pre>
    </div>

    <h3>Visualization</h3>
    <div class="method">
        <h4>to_tikz(tensor: Tensor) -> str</h4>
        <p>Converts tensor network to TikZ diagram.</p>
    </div>

    <div class="method">
        <h4>to_graphviz(tensor: Tensor) -> str</h4>
        <p>Converts tensor network to Graphviz DOT format.</p>
    </div>

</body>
</html>
